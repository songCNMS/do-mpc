seed: 42
num_of_seeds: 1
env_name: 'batch_reactor'
model_name: 'ppo'
normalize: True
dense_reward: True
debug_mode: False

# for online learning
online_training: False
buffer_maxlen: 1000000
explorer_start_epsilon: 1.0
explorer_end_epsilon: 0.1
explorer_duration: 20000
N_EPOCHS: 1000
n_steps_per_epoch: 100
online_random_steps: 600
online_update_interval: 512
online_save_interval: 1024

# for offline data generation and training
DYNAMICS_N_EPOCHS: 100
batch_size: 512
scaler: 'min_max' # None, 'pixel', 'min_max', 'standard'
action_scaler: 'min_max' # None, 'min_max'
reward_scaler: 'min_max' # None, 'min_max', 'standard'
evaluate_on_environment: True
default_loc: "d3rlpy_logs/"
plt_dir: "plt_results"
dataset_location: datasets # for dataset generation
training_dataset_loc: 'datasets/mpc_policy_batch_reactor_dataset_10000.h5'
eval_dataset_loc: 'datasets/mpc_policy_batch_reactor_dataset_10000.h5'

# env specific configs
reward_on_steady: True
reward_on_absolute_efactor: False
compute_diffs_on_reward: False
standard_reward_style: 'setpoint'
initial_state_deviation_ratio: 0.1
